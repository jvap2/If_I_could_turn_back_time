import numpy as np

import torch

from torch import nn


W_Layer_1 = np.array([[-0.0663131,-0.089169,-0.141303,-0.13197,-0.0859095,-0.105248,-0.107943,-0.0952576,-0.118739,-0.059964],

[-0.103081,-0.125729,-0.0804295,-0.149223,-0.107697,-0.137761,-0.124781,-0.112891,-0.0535421,-0.12478],

[-0.133324,-0.142538,-0.137327,-0.133104,-0.147943,-0.124381,-0.140337,-0.14836,-0.116688,-0.0997259],

[-0.0663968,-0.133001,-0.138895,-0.0576995,-0.114971,-0.0748044,-0.112948,-0.0729137,-0.120062,-0.0816867],

[-0.0828777,-0.0731428,-0.0574161,-0.113307,-0.0723656,-0.115113,-0.101069,-0.147147,-0.0780042,-0.104611],

[-0.121927,-0.0613281,-0.0971483,-0.109254,-0.144432,-0.0950918,-0.0836351,-0.134768,-0.0934513,-0.0503231],

[-0.0844943,-0.109848,-0.133324,-0.0733892,-0.117548,-0.098295,-0.0981936,-0.0804956,-0.121209,-0.0682556],

[-0.112182,-0.0540864,-0.0913984,-0.119598,-0.117394,-0.113764,-0.0847116,-0.0684622,-0.110911,-0.112716],

[-0.123073,-0.0828374,-0.124044,-0.0702213,-0.142091,-0.118476,-0.115313,-0.0757265,-0.103244,-0.0587644],

[-0.0760497,-0.137738,-0.118612,-0.059374,-0.0611276,-0.0861601,-0.107669,-0.109321,-0.116656,-0.0788778],

[-0.127577,-0.0788379,-0.0829642,-0.0689751,-0.148436,-0.0503579,-0.132739,-0.0831479,-0.0688201,-0.0936497],

[-0.145864,-0.141893,-0.126487,-0.119908,-0.0621143,-0.118579,-0.0883832,-0.127427,-0.144305,-0.141627],

[-0.136192,-0.0703548,-0.129366,-0.104804,-0.0797288,-0.140493,-0.140964,-0.137398,-0.0998144,-0.10762],

[-0.0662757,-0.0773911,-0.136458,-0.0992399,-0.0963662,-0.134894,-0.0995977,-0.0791053,-0.0680421,-0.118418],

[-0.122755,-0.0639058,-0.110311,-0.0992422,-0.133813,-0.122425,-0.0678208,-0.0721966,-0.0998525,-0.0621259],

[-0.0638238,-0.0860443,-0.0824807,-0.14319,-0.140848,-0.11221,-0.133683,-0.131813,-0.0996074,-0.0834972]])

b_Layer_1 = np.array([-0.894327, -1.15883, -1.10888, -0.758906, -0.65123, -0.572545, -0.607848, -1.14721, -0.863598, -0.78827, -0.831386, -0.591149, -0.927328, -1.43449, -1.08357, -0.765461])

W_Layer_2 = np.array([[-0.0550134,-0.0551354,-0.050338,-0.077338,-0.0475278,-0.0718537,-0.0657698,-0.0887244,-0.0741241,-0.0818616,-0.0748655,-0.0507469,-0.0716181,-0.0316253,-0.06456,-0.0839944],

[-0.0699029,-0.0714183,-0.0636572,-0.0562943,-0.0538846,-0.0761792,-0.0813685,-0.0736133,-0.0408047,-0.0333058,-0.0352225,-0.0741076,-0.042976,-0.0699349,-0.0750188,-0.0667394],

[-0.0313203,-0.0316068,-0.0503274,-0.0475981,-0.0722105,-0.0848472,-0.0425726,-0.0525847,-0.0729588,-0.0861881,-0.0720816,-0.0508268,-0.0865634,-0.0428915,-0.0410712,-0.0627163],

[-0.0830598,-0.0734784,-0.0877606,-0.0431945,-0.0559076,-0.0753792,-0.0855578,-0.0654623,-0.077435,-0.0895303,-0.0458199,-0.089161,-0.0657152,-0.0895887,-0.0621504,-0.0657855]])

b_Layer_2 = np.array([-1.43913, -1.29965, -1.31414, -1.0945])

inpt = np.array([0.440298, 0.550562, 0.195876, 0.110092, 0.333333, 0.535268, 0.0714286, 1, 0.4, 0 ])

class Network(nn.Module):
    def __init__(self):
        super(Network, self).__init__()
        self.layer1 = nn.Linear(10, 16)
        self.layer2 = nn.Linear(16, 4)
        self.layer1.weight = nn.Parameter(torch.tensor(W_Layer_1, dtype=torch.float32))
        self.layer1.bias = nn.Parameter(torch.tensor(b_Layer_1, dtype=torch.float32))
        self.layer2.weight = nn.Parameter(torch.tensor(W_Layer_2, dtype=torch.float32))
        self.layer2.bias = nn.Parameter(torch.tensor(b_Layer_2, dtype=torch.float32))
        self.layer1.requires_grad = True
        self.layer2.requires_grad = True

    def forward(self, x):
        x = torch.tensor(x, dtype=torch.float32)
        x = self.layer1(x)
        x= torch.relu(x)
        x = self.layer2(x)
        x = torch.softmax(x,dim=0)
        return x
    

model = Network()
model.train()
output=model(inpt)
print(output)

## Use categorical cross entropy loss

loss = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
target = torch.tensor([1.0,0.0,0.0,0.0])

## Perform backward pass

output = output.unsqueeze(0)
target = target.unsqueeze(0)
optimizer.zero_grad()
loss_val = loss(output, target)
loss_val.backward()
optimizer.step()

print(loss_val)



# # # Compute the gradients

## See the updated weights

print(model.layer1.weight)
print(model.layer1.bias)
print(model.layer2.weight)
print(model.layer2.bias)



print("Layer 1 Weights Gradient")
print(model.layer1.weight.grad)
print("Layer 1 bias Gradient")
print(model.layer1.bias.grad)
print("Layer 2 Weights Gradient")
print(model.layer2.weight.grad)
print("Layer 2 bias Gradient")
print(model.layer2.bias.grad)