import numpy as np

import torch

from torch import nn


W_Layer_1 = np.array([[-0.301304,-0.0968752,0.369422,0.285944,-0.126029,0.0469441,0.0710443,-0.0424175,0.167604,-0.358093],

[0.0275555,0.230131,-0.175044,0.440262,0.068845,0.337748,0.221647,0.115301,-0.415532,0.221642],

[0.298058,0.380468,0.333864,0.296089,0.428819,0.218071,0.360782,0.432541,0.149262,-0.00245205],

[-0.300556,0.295172,0.347886,-0.378347,0.133902,-0.225356,0.11581,-0.242267,0.17944,-0.163799],

[-0.153146,-0.240218,-0.380882,0.119023,-0.247169,0.135177,0.00955784,0.421692,-0.196736,0.0412392],

[0.19612,-0.345892,-0.025506,0.0827702,0.39741,-0.0439007,-0.146372,0.310978,-0.0585731,-0.444323],

[-0.138687,0.0880843,0.298062,-0.238014,0.15695,-0.0152497,-0.0161571,-0.174453,0.189697,-0.283931],

[0.108962,-0.410663,-0.0769353,0.175293,0.155574,0.123109,-0.136744,-0.282082,0.0975873,0.113733],

[0.206371,-0.153507,0.215055,-0.266349,0.376477,0.165251,0.136964,-0.217108,0.0290161,-0.368823],

[-0.214218,0.337542,0.166475,-0.36337,-0.347686,-0.123788,0.068594,0.0833709,0.148973,-0.188923],

[0.246654,-0.189279,-0.152373,-0.277495,0.433228,-0.444013,0.292828,-0.15073,-0.278881,-0.0567988],

[0.410217,0.374703,0.236908,0.178058,-0.33886,0.166172,-0.103904,0.245318,0.396277,0.372326],

[0.323709,-0.265155,0.262655,0.0429701,-0.181311,0.362183,0.366396,0.334497,-0.00166014,0.0681549],

[-0.30164,-0.20222,0.326089,-0.00679868,-0.0325015,0.312103,-0.00359792,-0.186888,-0.28584,0.164734],

[0.203527,-0.322836,0.0922233,-0.00677821,0.302436,0.200577,-0.28782,-0.248682,-0.00131887,-0.338756],

[-0.323569,-0.124824,-0.156697,0.386299,0.36536,0.109205,0.301268,0.284542,-0.00351116,-0.147606]
])

b_Layer_1 = np.array([-0.298888, 0.449242, 0.307967, -0.681916, -0.986471, -1.20903, -1.10917, 0.416365, -0.385802, -0.598864, -0.476913, -1.15641, -0.205548, 1.22894, 0.236372, -0.663376])

W_Layer_2 = np.array([[0.186509,0.194053,-0.152531,-0.299281,0.268707,-0.23142,-0.226991,-0.0991464,-0.0402749,-0.0857646,0.104314,-0.282358,-0.123241,0.261233,0.0760849,-0.279891],

[0.216225,0.176578,-0.0715765,-0.0941892,-0.0747845,-0.161086,0.0704592,-0.305304,0.283938,-0.0479425,0.269572,0.123637,-0.0278235,-0.0200544,-0.146773,-0.194868],

[-0.179555,0.0542499,-0.140595,-0.264401,0.176383,-0.0140322,-0.00999442,-0.217445,0.253757,-0.259234,-0.14625,-0.223037,-0.351554,0.283388,-0.149375,0.218225],

[0.106413,0.132602,-0.229518,-0.321925,0.325069,0.194495,-0.273676,0.255453,-0.207001,0.349449,0.0255369,0.118729,-0.0241585,0.232317,0.277415,0.14984],

[-0.0669861,-0.216733,0.238992,-0.244157,0.122788,-0.124556,-0.108049,0.022991,-0.0302361,0.099255,0.153507,-0.0282366,0.0290901,-0.349422,-0.163565,-0.21805],

[0.136733,-0.0395291,-0.186422,0.108249,-0.198587,-0.106544,0.0101482,-0.0520349,-0.110648,-0.317868,-0.286859,0.218747,0.268002,0.344109,0.0150337,-0.152537],

[-0.226178,-0.0995276,-0.0431403,0.250163,0.12947,0.202365,-0.080399,-0.254319,-0.0519338,-0.280445,0.0709974,0.33071,-0.276314,0.260986,-0.240894,0.213973],

[-0.132097,-0.0737624,-0.0313316,0.0228692,0.173247,0.33237,0.324388,-0.290954,-0.339052,-0.316025,0.281346,0.282504,-0.325469,-0.0571739,-0.223587,-0.198094],

[0.196852,0.0868266,-0.301484,-0.0272315,-0.0643622,-0.0283292,0.0720025,0.237257,0.0447788,-0.210554,0.214414,0.122018,-0.303121,0.327073,-0.0175618,-0.0816645],

[-0.100243,0.30466,0.294758,-0.280549,0.283477,0.265592,-0.21795,0.297978,0.303121,-0.290158,0.226929,0.331205,0.00622177,-0.350211,-0.220442,-0.15048],

[0.0901687,-0.168372,0.175842,-0.327747,0.156852,-0.105709,0.263064,-0.151922,0.0372911,0.123924,0.323649,0.0877234,0.0974437,-0.0474658,-0.347494,0.350754],

[-0.0963592,0.300817,-0.283348,-0.166436,0.212856,-0.147745,-0.222011,0.162424,-0.0843494,-0.348636,0.140075,0.275426,-0.345294,0.273187,-0.228607,0.0984282],

[-0.248739,0.300788,0.124235,0.261667,-0.158474,0.0337451,-0.243809,0.232371,-0.195884,-0.273713,-0.0334595,0.255113,0.0323745,-0.0274006,0.252314,0.289569],

[-0.0801369,0.322519,-0.230421,-0.220834,-0.178779,-0.0988788,0.295143,0.0904247,-0.0939614,0.0816645,0.0122971,-0.0857019,0.00129777,0.137243,-0.340827,0.106112],

[0.0844779,0.136961,0.0142258,0.279557,-0.182847,0.12397,0.158374,-0.025178,0.20381,-0.228638,-0.123618,-0.117369,0.0975144,-0.224858,-0.181353,-0.336176],

[-0.255892,-0.0582207,-0.203457,-0.0811182,0.196454,-0.261868,-0.344247,-0.251061,0.17335,0.0216037,0.0167905,-0.178905,-0.194707,0.0295168,0.28076,0.243325],

[-0.187076,-0.0585671,0.169329,-0.0163695,-0.28815,-0.0258504,0.312006,0.269213,0.0990646,-0.165166,-0.201709,-0.156974,-0.0364706,-0.0295086,-0.139597,0.0611906],

[0.265824,0.0104997,0.333626,0.108725,0.102186,0.342932,0.211217,-0.0780175,0.0109827,-0.125546,0.0966306,0.169829,0.257524,0.0238377,0.0596007,-0.283105],

[0.318824,-0.124624,0.0540791,-0.32288,0.203079,0.0125315,0.299887,-0.05141,0.200919,-0.255375,0.145169,-0.189105,0.0686696,-0.347981,0.225639,-0.0190597],

[0.0160717,0.205711,-0.263889,-0.235296,0.19509,0.300882,0.0402398,-0.14748,-0.178218,-0.216683,-0.331204,-0.274247,0.160708,0.0819497,-0.203798,0.125979],

[0.310879,0.203834,0.156652,0.160404,-0.137188,0.102986,-0.244559,-0.289822,0.201164,0.254163,-0.125374,-0.0837196,0.259735,-0.253288,0.250774,-0.0777461],

[0.305977,0.340439,0.0405112,0.147513,0.287767,-0.272802,-0.35352,-0.244004,-0.135932,-0.331171,-0.164697,-0.328777,0.104332,-0.0149418,0.150755,0.0616576],

[-0.164661,-0.0461458,-0.131491,0.0517049,-0.296713,-0.0224969,0.115436,0.258004,-0.121887,0.343616,-0.179269,-0.215705,-0.263226,-0.282048,0.0601026,-0.310803],

[-0.295163,-0.25294,0.190264,0.346158,-0.172189,0.190298,-0.251399,0.0454328,0.21268,-0.0625428,0.0702091,-0.0365416,0.276069,-0.132589,-0.328437,-0.242145],

[0.174818,-0.106375,0.163113,0.231658,0.224681,-0.0750042,0.136109,-0.250759,-0.0849416,0.310394,-0.11291,0.00538611,-0.325208,0.300746,0.048137,-0.266817],

[-0.305747,-0.115152,-0.274212,-0.124383,-0.278408,-0.172058,0.274604,0.287825,0.118953,-0.00874072,-0.10227,0.0414681,0.212224,-0.0771538,0.152876,0.0334885],

[0.170024,-0.0375643,-0.0884064,0.0411519,0.240985,-0.30585,0.143946,-0.19751,-0.34901,-0.322518,0.161429,-0.320664,0.331781,-0.143987,-0.233927,-0.327519],

[0.0944144,-0.154586,-0.0983487,0.16956,0.0269092,-0.177299,0.103832,-0.207692,0.167514,-0.351991,0.18733,0.0261843,-0.0755917,-0.0133473,-0.293881,-0.259121],

[0.302642,-0.0287335,0.135584,0.190073,0.0189695,-0.0740228,0.346117,0.0235131,-0.0429869,0.153993,0.0564026,-0.0647589,-0.343548,0.176029,-0.038725,0.10442],

[-0.332111,0.21648,-0.0795733,0.0483515,-0.314372,-0.329295,0.194213,0.206695,-0.327733,0.0279901,-0.120674,-0.0497709,-0.338911,-0.0610008,0.0446616,0.317285],

[0.263819,-0.173307,0.153805,-0.0707647,0.106223,0.146368,0.306302,-0.290317,-0.053193,0.00915095,-0.00152254,-0.0431873,-0.168374,0.313306,-0.292321,-0.146932],

[0.176232,-0.0183405,0.254973,0.215413,0.00591806,0.0956334,0.0685555,0.0317388,-0.22993,0.301435,0.335521,-0.215287,-0.113119,0.0266295,-0.251556,-0.202853]

])

b_Layer_2 = np.array([0.827502, 1.02321, 0.319742, -0.161818, 0.194466, 0.130735, 0.0911273, 1.39591, -1.24687, -1.32918, -0.191056, -0.506156, 1.33826, 0.0538753, 0.320331, 0.628976, 1.39473, -0.073989, 0.0764165, 0.00418532, -1.10567, -1.06357, -1.28307, -0.611175, -1.27205, -1.3552, -0.0581098, -0.31031, 0.16553, 0.34988, 0.292491, -0.421181])

W_Layer_3 = np.array([[0.0474439,0.241748,-0.151419,0.0376014,0.02813,-0.0836469,0.0536933,-0.246939,-0.225034,-0.038311,0.0109172,0.0274445,0.244529,-0.0791888,-0.101155,-0.190388,-0.0696949,0.211896,0.211329,-0.000361189,0.0870644,0.0363675,-0.166581,0.141707,-0.0915579,0.191464,-0.0278104,0.0779064,-0.0873969,0.121539,0.135203,0.210047],

[0.113287,0.233784,-0.00235173,-0.108583,-0.0998627,-0.198658,-0.105522,-0.0748963,0.0130306,0.155396,0.202548,0.00756004,-0.173793,-0.148606,0.067172,0.00651181,-0.186711,0.0285012,-0.243849,0.150354,-0.185131,-0.160431,0.0420606,-0.0266892,-0.218967,-0.23575,-0.198783,-0.0563637,0.135789,0.186421,-0.0963168,-0.000923693],

[0.170205,0.151332,0.140494,-0.179658,0.202673,-0.215028,-0.00455426,-0.0342963,0.190367,-0.052006,0.223264,-0.233426,0.0493876,0.0404358,0.0230859,0.112677,-0.181063,0.0292365,0.013031,-0.116194,0.118806,-0.194908,0.107116,0.149839,-0.180658,0.158334,-0.156525,0.205131,0.0947543,-0.00284143,-0.0457929,0.014959],

[-0.10151,-0.155299,0.085301,-0.148837,-0.120327,-0.169253,0.0668669,-0.17996,0.0287407,0.0401306,-0.163386,-0.171872,-0.169434,0.1097,0.190805,-0.100497,-0.111064,-0.0461636,0.033309,-0.242258,0.00892797,-0.109574,0.157581,0.0782697,-0.201241,-0.248943,0.0334005,0.143514,-0.00178491,0.237608,-0.0915275,0.146705],

[-0.167692,0.243774,0.247868,-0.0380191,-0.17548,0.0647353,0.0320208,0.103261,-0.145134,0.118635,0.181389,-0.0645677,-0.0216652,0.122195,0.0849357,0.117271,-0.173969,-0.131755,0.125014,0.0849591,0.0086703,0.0325947,-0.0867711,0.0574296,0.0336512,0.196629,-0.0490569,-0.218134,0.184237,0.109416,0.178571,-0.233455],

[0.103189,0.17644,-0.0214738,0.177709,-0.00882486,-0.239453,0.0309705,0.0960411,0.129182,-0.0376402,-0.218527,-0.142483,-0.165445,0.116409,0.224788,-0.0894143,0.234654,0.0998016,0.245545,-0.00667581,-0.117604,-0.0912262,-0.199246,0.166048,-0.144597,0.0016968,0.197914,-0.21036,-0.138888,0.126485,-0.193814,0.214301],

[0.0529252,0.0347118,0.142011,-0.2059,0.0452588,-0.0770187,0.140141,-0.0755593,0.135341,0.171615,0.0319574,0.219896,0.038024,0.00674543,-0.119519,0.0226779,-0.143453,-0.123974,-0.233998,-0.0110567,0.0348002,-0.183244,-0.0950092,0.140203,0.0684525,-0.147095,0.179844,0.179565,0.22939,0.236029,0.143866,0.0323151],

[0.0207411,0.0358773,0.0764155,-0.184,0.208859,-0.0334431,-0.00955944,0.0941997,-0.111828,-0.227602,0.0640955,0.176196,0.0291434,0.194577,-0.0511264,0.13569,-0.179397,-0.0351244,-0.125366,0.105403,0.0316314,0.0296244,-0.00439312,-0.149916,0.132529,-0.0745494,-0.220351,0.111919,-0.0885202,0.173515,-0.105766,0.182221],

[-0.0406075,0.22065,0.248221,-0.0817489,-0.0627934,-0.0113386,-0.237549,0.0753783,0.0110593,0.0765463,0.00157404,-0.209797,0.0211233,0.200448,0.175893,0.0917266,-0.0846767,-0.199473,-0.0528699,0.196955,0.0801512,0.192737,-0.202961,-0.0373197,-0.131813,-0.173312,-0.175401,0.0296673,-0.249797,-0.0311666,-0.0381117,-0.0404047],

[-0.060517,-0.0398909,0.127846,0.12669,0.19877,0.140297,-0.0479321,-0.0401702,-0.0331563,0.203642,3.25143e-05,0.237967,0.15409,-0.0740744,0.0796936,-0.180587,-0.0235476,-0.223176,-0.233632,-0.193396,0.219561,-0.186594,0.019284,-0.162252,-0.109906,0.0938832,0.117415,-0.109703,-0.187283,-0.170696,0.0998919,0.00219962],

[0.0394127,-0.0222616,-0.121111,-0.0118168,-0.131964,0.0809571,0.198013,0.0848795,0.034599,-0.0519546,0.0728465,-0.0613114,0.123971,-0.0974599,0.00810146,-0.149577,-0.0706363,0.024469,-0.0929728,-0.101076,0.0878752,0.176311,-0.0133277,0.227969,0.0201943,-0.145912,-0.131734,0.0829109,-0.0666086,0.218157,-0.164889,0.222804],

[-0.0541042,-0.0360003,-0.0390127,0.0639315,-0.205043,-0.0909998,-0.101189,0.0795558,0.107046,0.221657,-0.231756,-0.0189833,-0.125802,0.0263459,0.0814402,0.0535612,-0.199185,0.238467,0.202486,0.13869,0.164778,-0.0608422,0.116659,-0.0650272,0.0432456,0.234925,-0.232116,0.226637,0.203082,-0.147006,0.199441,-0.101022],

[0.066994,-0.0895716,0.212909,0.111951,0.0694286,-0.13828,-0.0584933,-0.0735258,-0.166622,-0.0402489,0.157491,-0.0424248,0.236097,-0.0110689,-0.238864,-0.213088,-0.0226016,0.213622,0.175602,-0.107823,-0.0972202,0.0422611,0.0771497,0.196025,0.0271857,0.0950334,0.172662,-0.0197323,0.198028,0.122103,0.129245,0.0150217],

[-0.217468,0.0921547,-0.123027,0.10196,0.203875,0.0684793,-0.221565,-0.212747,-0.22177,0.185925,-0.00517206,-0.235673,-0.0751435,0.00596437,-0.198761,0.152255,-0.0304137,0.226841,-0.205568,0.122366,0.0191023,0.121581,0.0683914,-0.203712,-0.0333851,-0.00894628,0.0265557,-0.0853574,-0.136843,-0.0941988,0.179664,-0.104311],

[0.247956,-0.193363,0.247649,0.201831,0.125116,-0.223916,0.239084,0.153347,0.212009,-0.0160884,0.167674,-0.113134,0.239876,0.218913,-0.21088,-0.0405377,0.195754,-0.166448,-0.168172,-0.0351432,0.205134,0.15022,0.0111448,-0.0782514,-0.108726,-0.212299,0.0863911,0.00443062,-0.0564983,0.0160554,0.150119,-0.0585423],

[0.0726923,0.147769,-0.106711,-0.0521916,0.173852,-0.117628,-0.148845,0.135861,0.116284,-0.231171,-0.227273,0.10616,0.237742,-0.188153,-0.184378,0.183497,-0.1046,-0.102549,-0.101647,-0.149467,-0.20233,0.159498,0.0222819,-0.0610561,0.197199,-0.141327,0.193375,-0.1093,0.124729,0.093494,0.0821581,-0.0525792]

])

b_Layer_3 = np.array([-0.0494255, 1.27532, 0.82154, -0.480182, -0.804297, 1.39376, -1.12585, 1.26772, -1.32816, -0.997283, 0.454035, -1.3975, -0.647421, 0.825251, 1.05473, 0.175084])


W_Layer_4 = np.array([[-0.143663,-0.0147291,-0.337876,-0.260937,0.353307,0.34156,-0.098204,-0.291482,-0.0956753,0.1887,-0.209872,-0.273275,-0.110688,0.210928,-0.157054,-0.329243],

[-0.101882,-0.315699,-0.0398829,-0.0956055,0.288011,0.330126,-0.221627,0.260199,-0.292304,-0.104294,0.160699,-0.0231295,-0.0831157,0.0596949,-0.117934,0.126775],

[-0.308588,-0.102257,0.219391,-0.308834,-0.11425,-0.232366,-0.246763,0.143628,0.309888,-0.103082,0.223906,-0.154354,-0.245707,-0.286701,-0.130043,0.00596386],

[-0.248847,0.183627,0.263912,-0.314389,0.1602,-0.311269,0.299363,0.221449,-0.0620089,0.106509,-0.155233,0.208429,-0.18735,0.0803863,-0.0183496,-0.142384]])

b_Layer_4 = np.array([1.32673, -0.610046, -0.390658, -0.544482])

inpt = np.array([0.335821, 0.674157, 0.216495, 0.66055, 0.333333, 0.481019, 0.142857, 0, 0.2, 0.5 ], dtype=np.float32)

gt_1 = np.array([0.0,0.0,1.0,0.0], dtype=np.float32)

inpt_2 = np.array([0.365672, 0.505618, 0.14433, 0.00917431, 0.666667, 0.559122, 0.5, 1, 0.4, 0 ], dtype=np.float32)

gt_2 = np.array([0.0,1.0,0.0,0.0], dtype=np.float32)

class Network(nn.Module):
    def __init__(self):
        super(Network, self).__init__()
        self.layer1 = nn.Linear(10, 16)
        self.layer2 = nn.Linear(16, 32)
        self.layer3 = nn.Linear(32, 16)
        self.layer4 = nn.Linear(16, 4)
        self.layer1.weight = nn.Parameter(torch.tensor(W_Layer_1, dtype=torch.float32))
        self.layer1.bias = nn.Parameter(torch.tensor(b_Layer_1, dtype=torch.float32))
        self.layer2.weight = nn.Parameter(torch.tensor(W_Layer_2, dtype=torch.float32))
        self.layer2.bias = nn.Parameter(torch.tensor(b_Layer_2, dtype=torch.float32))
        self.layer3.weight = nn.Parameter(torch.tensor(W_Layer_3, dtype=torch.float32))
        self.layer3.bias = nn.Parameter(torch.tensor(b_Layer_3, dtype=torch.float32))
        self.layer4.weight = nn.Parameter(torch.tensor(W_Layer_4, dtype=torch.float32))
        self.layer4.bias = nn.Parameter(torch.tensor(b_Layer_4, dtype=torch.float32))
        self.layer1.requires_grad = True
        self.layer2.requires_grad = True
        self.layer3.requires_grad = True
        self.layer4.requires_grad = True
        self.sm = nn.Softmax(dim=0)
        self.relu_1 = nn.ReLU()
        self.relu_2 = nn.ReLU()
        self.relu_3 = nn.ReLU()

    def forward(self, x):
        x = torch.tensor(x, dtype=torch.float32, requires_grad=True)
        x = self.layer1(x)
        
        x= self.relu_1(x)
        
        x = self.layer2(x)
        
        x= self.relu_2(x)
        
        x = self.layer3(x)

        x = self.relu_3(x)

        x = self.layer4(x)
        
        x = self.sm(x)
        
        return x
    
def hook_fn(module, grad_input, grad_output):
    print("Gradient output:", grad_output)
    

model = Network()


# model.sm.register_backward_hook(hook_fn)
# model.relu_1.register_backward_hook(hook_fn)
# model.relu_2.register_backward_hook(hook_fn)
loss = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
model.train()
output=model(inpt)
print(output)

## Use categorical cross entropy loss


## Perform backward pass

output = output.unsqueeze(0)
target = torch.tensor(gt_1).unsqueeze(0)
optimizer.zero_grad()
loss_val = loss(output, target)
loss_val.backward()
optimizer.step()

output_2 = model(inpt_2)
print(output_2)
output_2 = output_2.unsqueeze(0)
target_2 = torch.tensor(gt_2).unsqueeze(0)
optimizer.zero_grad()
loss_val = loss(output_2, target_2)
loss_val.backward()
optimizer.step()



# print(loss_val)



# # # # Compute the gradients

# ## See the updated weights

# print(model.layer1.weight)
# print(model.layer1.bias)
# print(model.layer2.weight)
# print(model.layer2.bias)
# print(model.layer3.weight)
# print(model.layer3.bias)



# print("Layer 1 Weights Gradient")
# print(model.layer1.weight.grad)
# print("Layer 1 bias Gradient")
# print(model.layer1.bias.grad)
# print("Layer 2 Weights Gradient")
# print(model.layer2.weight.grad)
# print("Layer 2 bias Gradient")
# print(model.layer2.bias.grad)
# print("Layer 3 Weights Gradient")
# print(model.layer3.weight.grad)
# print("Layer 3 bias Gradient")
# print(model.layer3.bias.grad)


